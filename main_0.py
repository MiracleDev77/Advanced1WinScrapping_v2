import argparse
import numpy as np
import pandas as pd
import joblib
import tensorflow as tf
from data.database import CasinoDatabase
from data.preprocessor import DataPreprocessor
from training.trainer import CasinoTrainer, TemporalFeatureBuilder
from training.evaluator import CasinoEvaluator
from config.paths import Paths
from config.params import TrainingParams, DataParams
from models.lstm_attention import LSTMModel
from models.xgboost import XGBoostModel, XGBRegressorModel

def main():
    # Configuration des arguments en ligne de commande
    parser = argparse.ArgumentParser(description="Syst√®me de pr√©diction pour donn√©es de casino")
    parser.add_argument('--train', action='store_true', help='Lancer l\'entra√Ænement complet (LSTM + XGBoost)')
    parser.add_argument('--evaluate', action='store_true', help='√âvaluer les mod√®les entra√Æn√©s')
    parser.add_argument('--predict', nargs='?', const='last', 
                        help='Faire des pr√©dictions (optionnel: sp√©cifier "last" pour les derni√®res donn√©es ou un nombre de jours)')
    parser.add_argument('--cross-validate', action='store_true', 
                        help='Lancer une validation crois√©e compl√®te')
    args = parser.parse_args()

    # Activation des optimisations TensorFlow
    tf.config.optimizer.set_jit(True)  # Activer XLA
    print(f"XLA activ√©: {tf.config.optimizer.get_jit()}")
    
    # V√©rification de la disponibilit√© GPU
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"GPU d√©tect√©: {len(gpus)} dispositif(s)")
    else:
        print("‚ö†Ô∏è Aucun GPU d√©tect√© - L'entra√Ænement sera plus lent")

    # Initialisation de la base de donn√©es
    db = CasinoDatabase()
    
    # Mode entra√Ænement
    if args.train or args.cross_validate:
        # Charger toutes les donn√©es historiques
        raw_data = db.fetch_data()
        print(f"üìä Donn√©es charg√©es: {len(raw_data)} enregistrements")
        
        # Validation crois√©e
        if args.cross_validate:
            print(f"\nüîÅ Lancement de la validation crois√©e ({TrainingParams.NUM_FOLDS} folds)")
            for fold in range(TrainingParams.NUM_FOLDS):
                print(f"\n‚è≥ Entra√Ænement fold {fold+1}/{TrainingParams.NUM_FOLDS}")
                trainer = CasinoTrainer(fold=fold)
                models = trainer.train(raw_data)
                
                # √âvaluation imm√©diate apr√®s entra√Ænement
                if models and models[0] is not None:
                    evaluate_fold(trainer, models, raw_data, fold)
        
        # Entra√Ænement simple
        elif args.train:
            print("\n‚è≥ Lancement de l'entra√Ænement sur le dataset complet")
            trainer = CasinoTrainer()
            models = trainer.train(raw_data)
            
            # √âvaluation imm√©diate apr√®s entra√Ænement
            if models and models[0] is not None:
                evaluate_fold(trainer, models, raw_data)

    # Mode √©valuation
    if args.evaluate:
        print("\nüîç √âvaluation des mod√®les entra√Æn√©s")
        evaluate_trained_models(db)

    # Mode pr√©diction
    if args.predict:
        print("\nüîÆ G√©n√©ration de pr√©dictions")
        predict_future(db, args.predict)


def evaluate_fold(trainer, models, raw_data, fold=None):
    """√âvalue les mod√®les d'un fold sp√©cifique"""
    # Charger les donn√©es de validation
    _, val_raw, _ = trainer.preprocessor.split_data(raw_data)
    val = trainer.preprocessor.transform(val_raw)
    X_val, y_val, val_scores = trainer.preprocessor.prepare_sequences(val)
    y_val_class, y_val_period = y_val
    
    # Faire des pr√©dictions avec LSTM
    lstm_pred = models[0].model.predict(X_val, verbose=0)
    y_pred_class_lstm = np.argmax(lstm_pred[0], axis=1)
    y_pred_period_lstm = lstm_pred[1].flatten()
    pred_scores_lstm = lstm_pred[2].flatten()
    
    # Faire des pr√©dictions avec XGBoost
    feature_builder = TemporalFeatureBuilder(DataParams.WINDOW_SIZE)
    X_val_feat = feature_builder.extract_hybrid_features(models[0].model, X_val, val)
    
    # CORRECTION: Formatage correct des pr√©dictions
    y_pred_class_xgb = models[1].model.predict(X_val_feat)
    y_pred_class_xgb = np.array(y_pred_class_xgb).ravel()  # S'assurer que c'est un vecteur 1D
    
    # CORRECTION: Application du seuil pour les pr√©dictions p√©riodiques
    y_pred_period_xgb = (models[2].model.predict(X_val_feat) > 0.5).astype(int)
    y_pred_period_xgb = np.array(y_pred_period_xgb).ravel()
    
    pred_scores_xgb = models[3].model.predict(X_val_feat)
    y_proba_class_xgb = models[1].model.predict_proba(X_val_feat)
    
    # CORRECTION: Troncature des pr√©dictions √† la taille de la v√©rit√© terrain
    min_length = min(len(y_val_class), len(y_pred_class_xgb))
    y_val_class = y_val_class[:min_length]
    y_val_period = y_val_period[:min_length]
    val_scores = val_scores[:min_length]
    y_pred_class_xgb = y_pred_class_xgb[:min_length]
    y_pred_period_xgb = y_pred_period_xgb[:min_length]
    pred_scores_xgb = pred_scores_xgb[:min_length]
    y_proba_class_xgb = y_proba_class_xgb[:min_length]
    
    # Cr√©er l'√©valuateur
    evaluator = CasinoEvaluator(fold=fold)
    
    # √âvaluation LSTM
    print("\nüìà √âvaluation LSTM:")
    report_lstm = evaluator.evaluate(
        y_val_class, y_val_period, val_scores,
        y_pred_class_lstm, y_pred_period_lstm, pred_scores_lstm, lstm_pred[0]
    )
    
    # √âvaluation XGBoost
    print("\nüìà √âvaluation XGBoost:")
    report_xgb = evaluator.evaluate(
        y_val_class, y_val_period, val_scores,
        y_pred_class_xgb, y_pred_period_xgb, pred_scores_xgb, 
        y_proba_class_xgb
    )
    
    # √âvaluation de l'ensemble (moyenne)
    print("\nüìà √âvaluation de l'ensemble (moyenne):")
    y_pred_class_ens = (y_pred_class_lstm + y_pred_class_xgb) // 2
    y_pred_period_ens = ((y_pred_period_lstm + y_pred_period_xgb) / 2 > 0.5).astype(int)
    pred_scores_ens = (pred_scores_lstm + pred_scores_xgb) / 2
    y_proba_class_ens = (lstm_pred[0] + y_proba_class_xgb) / 2
    
    # Troncature pour l'ensemble
    min_length_ens = min(len(y_val_class), len(y_pred_class_ens))
    y_val_class_ens = y_val_class[:min_length_ens]
    y_val_period_ens = y_val_period[:min_length_ens]
    val_scores_ens = val_scores[:min_length_ens]
    y_pred_class_ens = y_pred_class_ens[:min_length_ens]
    y_pred_period_ens = y_pred_period_ens[:min_length_ens]
    pred_scores_ens = pred_scores_ens[:min_length_ens]
    y_proba_class_ens = y_proba_class_ens[:min_length_ens]
    
    report_ens = evaluator.evaluate(
        y_val_class_ens, y_val_period_ens, val_scores_ens,
        y_pred_class_ens, y_pred_period_ens, pred_scores_ens, 
        y_proba_class_ens
    )


def evaluate_trained_models(db):
    """√âvalue les mod√®les pr√©-entra√Æn√©s sur les donn√©es r√©centes"""
    try:
        # Charger le pr√©processeur
        preprocessor = joblib.load(Paths.PREPROCESSOR)
        
        # Charger les derni√®res donn√©es
        test_data = db.fetch_data(last_n=365)  # Derni√®re ann√©e
        test_data = preprocessor.transform(test_data)
        
        # Pr√©parer les s√©quences
        X_test, y_test, test_scores = preprocessor.prepare_sequences(test_data)
        if len(X_test) == 0:
            print("‚ö†Ô∏è Aucune donn√©e de test disponible")
            return
            
        y_test_class, y_test_period = y_test
        
        # Charger les mod√®les
        lstm_model = LSTMModel((DataParams.WINDOW_SIZE, len(preprocessor.feature_columns))).load()
        xgb_clf = XGBoostModel.load(str(Paths.XGB_MODEL), n_classes=3)
        xgb_period = XGBoostModel.load(str(Paths.XGB_PERIOD), n_classes=1, objective='binary:logistic')
        xgb_reg = XGBRegressorModel.load(str(Paths.XGB_REGRESSOR))
        
        # Cr√©er les features hybrides
        feature_builder = TemporalFeatureBuilder(DataParams.WINDOW_SIZE)
        X_test_feat = feature_builder.extract_hybrid_features(lstm_model.model, X_test, test_data)
        
        # Faire des pr√©dictions
        lstm_pred = lstm_model.model.predict(X_test, verbose=0)
        y_pred_class_lstm = np.argmax(lstm_pred[0], axis=1)
        y_pred_period_lstm = lstm_pred[1].flatten()
        pred_scores_lstm = lstm_pred[2].flatten()
        
        # CORRECTION: Formatage correct des pr√©dictions
        y_pred_class_xgb = xgb_clf.model.predict(X_test_feat)
        y_pred_class_xgb = np.array(y_pred_class_xgb).ravel()  # S'assurer que c'est un vecteur 1D
        
        y_pred_period_xgb = xgb_period.model.predict(X_test_feat)
        y_pred_period_xgb = np.array(y_pred_period_xgb).ravel()  # S'assurer que c'est un vecteur 1D
        
        pred_scores_xgb = xgb_reg.model.predict(X_test_feat)
        y_proba_class_xgb = xgb_clf.model.predict_proba(X_test_feat)
        
        # √âvaluation
        evaluator = CasinoEvaluator()
        
        print("\nüîç √âvaluation LSTM sur donn√©es r√©centes:")
        evaluator.evaluate(
            y_test_class, y_test_period, test_scores,
            y_pred_class_lstm, y_pred_period_lstm, pred_scores_lstm, lstm_pred[0]
        )
        
        print("\nüîç √âvaluation XGBoost sur donn√©es r√©centes:")
        evaluator.evaluate(
            y_test_class, y_test_period, test_scores,
            y_pred_class_xgb, y_pred_period_xgb, pred_scores_xgb, 
            y_proba_class_xgb
        )
    
    except Exception as e:
        print(f"‚ùå Erreur lors de l'√©valuation: {e}")
        import traceback
        traceback.print_exc()
        print("Assurez-vous d'avoir d'abord entra√Æn√© les mod√®les avec --train")

def predict_future(db, period="last"):
    """G√©n√®re des pr√©dictions pour les donn√©es futures"""
    try:
        # Charger les artefacts
        preprocessor = joblib.load(Paths.PREPROCESSOR)
        lstm_model = LSTMModel((DataParams.WINDOW_SIZE, len(preprocessor.feature_columns))).load()
        xgb_clf = XGBoostModel.load(str(Paths.XGB_MODEL), n_classes=3)
        xgb_period = XGBoostModel.load(str(Paths.XGB_PERIOD), n_classes=1, objective='binary:logistic')
        xgb_reg = XGBRegressorModel.load(str(Paths.XGB_REGRESSOR))
        
        # D√©terminer les donn√©es √† pr√©dire
        if period == "last":
            # Derni√®res donn√©es disponibles
            prediction_data = db.fetch_data(last_n=DataParams.WINDOW_SIZE + 1)
            print(f"üîÆ Pr√©diction sur les {DataParams.WINDOW_SIZE + 1} derniers enregistrements")
        else:
            try:
                days = int(period)
                prediction_data = db.fetch_data(last_n=days)
                print(f"üîÆ Pr√©diction sur les {days} derniers jours")
            except:
                prediction_data = db.fetch_data()
                print("üîÆ Pr√©diction sur toutes les donn√©es disponibles")
        
        if len(prediction_data) < DataParams.WINDOW_SIZE + 1:
            print(f"‚ö†Ô∏è Donn√©es insuffisantes pour la pr√©diction (min: {DataParams.WINDOW_SIZE + 1} enregistrements)")
            return
        
        # Transformation des donn√©es
        prediction_data = preprocessor.transform(prediction_data)
        
        # Pr√©paration des s√©quences
        X_pred, _, _ = preprocessor.prepare_sequences(prediction_data)
        
        if len(X_pred) == 0:
            print("‚ö†Ô∏è Aucune s√©quence valide pour la pr√©diction")
            return
        
        # Cr√©er les features hybrides
        feature_builder = TemporalFeatureBuilder(DataParams.WINDOW_SIZE)
        X_pred_feat = feature_builder.extract_hybrid_features(lstm_model.model, X_pred, prediction_data)
        
        # Pr√©dictions LSTM
        lstm_pred = lstm_model.model.predict(X_pred, verbose=0)
        class_proba_lstm = lstm_pred[0]
        class_pred_lstm = np.argmax(class_proba_lstm, axis=1)
        period_pred_lstm = (lstm_pred[1].flatten() > 0.5).astype(int)
        score_pred_lstm = lstm_pred[2].flatten()
        
        # Pr√©dictions XGBoost
        class_pred_xgb = xgb_clf.model.predict(X_pred_feat)
        class_pred_xgb = np.array(class_pred_xgb).ravel()  # Format 1D
        
        class_proba_xgb = xgb_clf.model.predict_proba(X_pred_feat)
        period_pred_xgb = (xgb_period.model.predict(X_pred_feat) > 0.5).astype(int)
        score_pred_xgb = xgb_reg.model.predict(X_pred_feat)
        
        # Pr√©dictions d'ensemble (moyenne)
        class_pred_ens = (class_pred_lstm + class_pred_xgb) // 2
        period_pred_ens = ((period_pred_lstm + period_pred_xgb) > 1).astype(int)
        score_pred_ens = (score_pred_lstm + score_pred_xgb) / 2
        
        # Cr√©ation du DataFrame de r√©sultats
        results = prediction_data.iloc[DataParams.WINDOW_SIZE:].copy()
        results['Pred_Class_LSTM'] = class_pred_lstm
        results['Pred_Class_XGB'] = class_pred_xgb
        results['Pred_Class_Ens'] = class_pred_ens
        results['Pred_Period_LSTM'] = period_pred_lstm
        results['Pred_Period_XGB'] = period_pred_xgb
        results['Pred_Period_Ens'] = period_pred_ens
        results['Pred_Score_LSTM'] = score_pred_lstm
        results['Pred_Score_XGB'] = score_pred_xgb
        results['Pred_Score_Ens'] = score_pred_ens
        
        # Sauvegarde des r√©sultats
        prediction_file = Paths.PREDICTIONS / f'predictions_{pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")}.csv'
        results.to_csv(prediction_file, index=False)
        print(f"‚úÖ Pr√©dictions sauvegard√©es dans: {prediction_file}")
        
        # Affichage des derni√®res pr√©dictions
        print("\nüîî Derni√®res pr√©dictions:")
        print(results[['Date', 'Heure', 'Score', 'Pred_Score_Ens', 
                      'ScoreClass', 'Pred_Class_Ens', 
                      'Period', 'Pred_Period_Ens']].tail())
    
    except Exception as e:
        print(f"‚ùå Erreur lors de la pr√©diction: {e}")
        import traceback
        traceback.print_exc()
        print("Assurez-vous d'avoir d'abord entra√Æn√© les mod√®les avec --train")

if __name__ == "__main__":
    main()